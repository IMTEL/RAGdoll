services:
  # Reverse proxy handling HTTPS with NTNU cert
  nginx:
    image: nginx:1.27-alpine  # Pinned version with Alpine for reduced size
    container_name: ragdoll-nginx
    depends_on:
      - backend-service
      - frontend-service
      - chat-service
    ports:
      - "80:80"
    volumes:
      # Mount static config (use nginx.local.conf for local dev, nginx.conf for production)
      - ./nginx.local.conf:/etc/nginx/conf.d/default.conf:ro
    # Resource limits for production stability
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          cpus: "0.5"
          memory: 256M
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # FastAPI backend
  backend-service:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ragdoll-backend
    expose:
      - "8000"  # internal only, Nginx connects here
    environment:
      - ENV=dev
      # MongoDB Atlas connection (supports vector search)
      - MONGODB_URI=${MONGODB_URI}
      - MONGODB_DATABASE=${MONGODB_DATABASE}
      - MONGODB_CONTEXT_COLLECTION=${MONGODB_CONTEXT_COLLECTION}
      - MONGODB_AGENT_COLLECTION=${MONGODB_AGENT_COLLECTION}
      - MONGODB_DOCUMENTS_COLLECTION=${MONGODB_DOCUMENTS_COLLECTION}
      - MONGODB_USER_COLLECTION=${MONGODB_USER_COLLECTION}
      - RAG_DATABASE_SYSTEM=mongodb
      # LLM API Keys
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GPT_MODEL=${GPT_MODEL:-gpt-4o-mini}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - GEMINI_MODEL=${GEMINI_MODEL:-gemini-2.0-flash-lite}
      - IDUN_API_KEY=${IDUN_API_KEY}
      - IDUN_MODEL=${IDUN_MODEL:-openai/gpt-oss-120b}
      - IDUN_API_URL=${IDUN_API_URL:-https://idun-llm.hpc.ntnu.no/api/chat/completions}
      # Access Service
      - ACCESS_SERVICE=${ACCESS_SERVICE:-service}
      - FERNET_KEY=${FERNET_KEY}
    # Resource limits for production stability
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "1.0"
          memory: 1G
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  frontend-service:
    build:
      context: ../RAGdollConfig
      dockerfile: Dockerfile
    container_name: ragdoll-frontend
    environment:
      - NODE_ENV=development
    env_file:
      - ../RAGdollConfig/.env
    expose:
      - "3000"
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  chat-service:
    build:
      context: ../RAGdollChat
      dockerfile: Dockerfile
    container_name: ragdoll-chat
    expose:
      - "3000"
    env_file:
      - ../RAGdollChat/.env
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"


# Note: MongoDB Atlas is used as external service (cloud-hosted)
# No local MongoDB container needed for production
